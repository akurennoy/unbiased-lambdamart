{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unbiasedlambdamart.calculator import Calculator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing get_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unbiasedlambdamart.lambdaobj import get_unbiased_gradients_fixed_t\n",
    "from unbiasedlambdamart.objective import MIN_ARG, MAX_ARG\n",
    "from unbiasedlambdamart.objective import DatasetWithCalculatorRanks0AndT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = [1, 0, 0, 1, 0]\n",
    "preds = [0.4, 0.5, 0.8, 0.1, -1.0]\n",
    "ranks0 = [1, 2, 0, 0, 1]\n",
    "groups = [3, 2]\n",
    "discounts = [1, 1 / np.log2(3), 1 / np.log2(4)]\n",
    "t_plus = [1, 0.5, 1 / 3]\n",
    "\n",
    "def get_grad(pred_i, pred_j, rank0_i, pos_i, pos_j):\n",
    "    abs_delta_ndcg = np.abs(discounts[pos_i] - discounts[pos_j])\n",
    "    abs_delta_ndcg /= (0.01 + np.abs(pred_i - pred_j))\n",
    "    return -2.0 / (1 + np.exp(2.0 * (pred_i - pred_j))) * abs_delta_ndcg / t_plus[rank0_i]\n",
    "\n",
    "dataset = DatasetWithCalculatorRanks0AndT(\n",
    "    3,\n",
    "    np.array(ranks0) + 1,\n",
    "    1.0,\n",
    "    data=np.zeros((5, 1)),\n",
    "    label=c,\n",
    "    group=groups,\n",
    "    free_raw_data=False\n",
    ")\n",
    "grad = np.zeros(len(preds))\n",
    "hess = np.zeros(len(preds))\n",
    "get_unbiased_gradients_fixed_t(\n",
    "    np.ascontiguousarray(dataset.label, dtype=np.double), \n",
    "    np.ascontiguousarray(preds),\n",
    "    np.ascontiguousarray(dataset.t_plus),\n",
    "    np.ascontiguousarray(dataset.t_minus),\n",
    "    np.ascontiguousarray(dataset.ranks0),\n",
    "    len(preds),\n",
    "    np.ascontiguousarray(groups),\n",
    "    np.ascontiguousarray(dataset.calculator.query_boundaries),\n",
    "    len(dataset.calculator.query_boundaries) - 1,\n",
    "    np.ascontiguousarray(dataset.calculator.discounts),\n",
    "    np.ascontiguousarray(dataset.calculator.inverse_max_dcgs),\n",
    "    np.ascontiguousarray(dataset.calculator.sigmoids),\n",
    "    len(dataset.calculator.sigmoids),\n",
    "    MIN_ARG,\n",
    "    MAX_ARG,\n",
    "    dataset.calculator.idx_factor,\n",
    "    np.ascontiguousarray(grad), \n",
    "    np.ascontiguousarray(hess)\n",
    ")\n",
    "\n",
    "assert np.allclose(\n",
    "    grad,\n",
    "    [\n",
    "        get_grad(0.4, 0.5, 1, 2, 1) + get_grad(0.4, 0.8, 1, 2, 0),\n",
    "        -get_grad(0.4, 0.5, 1, 2, 1),\n",
    "        -get_grad(0.4, 0.8, 1, 2, 0),\n",
    "        get_grad(0.1, -1.0, 0, 1, 0),\n",
    "        -get_grad(0.1, -1.0, 0, 1, 0)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unbiasedlambdamart.lambdaobj import get_unbiased_gradients\n",
    "from unbiasedlambdamart.objective import DatasetWithCalculatorRanks0AndP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = [1, 0, 0, 1, 0]\n",
    "preds = [0.4, 0.5, 0.8, 0.1, -1.0]\n",
    "ranks0 = [1, 2, 0, 0, 1]\n",
    "groups = [3, 2]\n",
    "discounts = [1, 1 / np.log2(3), 1 / np.log2(4)]\n",
    "t_plus = np.array([1, 0.5, 1 / 3])\n",
    "t_minus = np.ones(3)\n",
    "t_plus_copy = t_plus.copy()\n",
    "t_minus_copy = t_minus.copy()\n",
    "\n",
    "def get_grad(pred_i, pred_j, rank0_i, rank0_j, pos_i, pos_j):\n",
    "    abs_delta_ndcg = np.abs(discounts[pos_i] - discounts[pos_j])\n",
    "    abs_delta_ndcg /= (0.01 + np.abs(pred_i - pred_j))\n",
    "    return -2.0 / (1 + np.exp(2.0 * (pred_i - pred_j))) * abs_delta_ndcg / t_plus[rank0_i] / t_minus[rank0_j]\n",
    "\n",
    "def get_t(pred_i, pred_j, rank0_i, rank0_j, pos_i, pos_j, t_plus, t_minus):\n",
    "    abs_delta_ndcg = np.abs(discounts[pos_i] - discounts[pos_j])\n",
    "    abs_delta_ndcg /= (0.01 + np.abs(pred_i - pred_j))\n",
    "    loss = np.log(1 + np.exp(-2.0 * (pred_i - pred_j))) * abs_delta_ndcg \n",
    "    return loss / t_minus[rank0_j], loss / t_plus[rank0_i]\n",
    "\n",
    "dataset = DatasetWithCalculatorRanks0AndP(\n",
    "    3,\n",
    "    np.array(ranks0) + 1,\n",
    "    1.0,\n",
    "    data=np.zeros((5, 1)),\n",
    "    label=c,\n",
    "    group=groups,\n",
    "    free_raw_data=False\n",
    ")\n",
    "grad = np.zeros(len(preds))\n",
    "hess = np.zeros(len(preds))\n",
    "\n",
    "get_unbiased_gradients(\n",
    "    np.ascontiguousarray(dataset.label, dtype=np.double), \n",
    "    np.ascontiguousarray(preds),\n",
    "    np.ascontiguousarray(dataset.ranks0),\n",
    "    len(preds),\n",
    "    np.ascontiguousarray(groups),\n",
    "    np.ascontiguousarray(dataset.calculator.query_boundaries),\n",
    "    len(dataset.calculator.query_boundaries) - 1,\n",
    "    np.ascontiguousarray(dataset.calculator.discounts),\n",
    "    np.ascontiguousarray(dataset.calculator.inverse_max_dcgs),\n",
    "    np.ascontiguousarray(dataset.calculator.sigmoids),\n",
    "    len(dataset.calculator.sigmoids),\n",
    "    MIN_ARG,\n",
    "    MAX_ARG,\n",
    "    dataset.calculator.idx_factor,\n",
    "    np.ascontiguousarray(dataset.calculator.logs),\n",
    "    len(dataset.calculator.logs),\n",
    "    MIN_ARG,\n",
    "    MAX_ARG,\n",
    "    dataset.calculator.idx_factor,\n",
    "    dataset.p,\n",
    "    dataset.calculator.max_rank,\n",
    "    np.ascontiguousarray(grad), \n",
    "    np.ascontiguousarray(hess),\n",
    "    np.ascontiguousarray(t_plus_copy),\n",
    "    np.ascontiguousarray(t_minus_copy)\n",
    ")\n",
    "\n",
    "assert np.allclose(\n",
    "    grad,\n",
    "    [\n",
    "        get_grad(0.4, 0.5, 1, 2, 2, 1) + get_grad(0.4, 0.8, 1, 0, 2, 0),\n",
    "        -get_grad(0.4, 0.5, 1, 2, 2, 1),\n",
    "        -get_grad(0.4, 0.8, 1, 0, 2, 0),\n",
    "        get_grad(0.1, -1.0, 0, 1, 1, 0),\n",
    "        -get_grad(0.1, -1.0, 0, 1, 1, 0)\n",
    "    ]\n",
    ")\n",
    "\n",
    "new_t_plus = np.zeros(len(t_plus))\n",
    "new_t_minus = np.zeros(len(t_minus))\n",
    "tp012, tm012 = get_t(0.4, 0.5, 1, 2, 2, 1, t_plus, t_minus)\n",
    "tp010, tm010 = get_t(0.4, 0.8, 1, 0, 2, 0, t_plus, t_minus)\n",
    "tp101, tm101 = get_t(0.1, -1.0, 0, 1, 1, 0, t_plus, t_minus)\n",
    "new_t_plus[1] += tp012\n",
    "new_t_minus[2] += tm012\n",
    "new_t_plus[1] += tp010\n",
    "new_t_minus[0] += tm010\n",
    "new_t_plus[0] += tp101\n",
    "new_t_minus[1] += tm101\n",
    "new_t_plus = np.power(new_t_plus / new_t_plus[0], 1 / 2)\n",
    "new_t_minus = np.power(new_t_minus / new_t_minus[0], 1 / 2)\n",
    "\n",
    "assert np.allclose(new_t_plus, t_plus_copy)\n",
    "assert np.allclose(new_t_minus, t_minus_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n_positions, coef, n_requests):\n",
    "    \"\"\"\n",
    "    This function is used for simulating the data. We generate n_requests result pages\n",
    "    with n_positions positions each. A logistic regression model is used for generating \n",
    "    interactions. The coefficients are provided via the coef parameter. The number of\n",
    "    features is inferred from its length. Features are simulated as standard normal random\n",
    "    variables. Position biases are set to 1/k, where k is the position number.\n",
    "    :param page_len: the number of positions on each result page\n",
    "    :param coef: a matrix defining the two logistic regression models for generating\n",
    "                 interactions\n",
    "    :param n_requests: the number of requests/queries/result pages\n",
    "    :param position_biases\n",
    "    :returns: a pandas.DataFrame having n_requests * n_positions rows \n",
    "              and the following columns:\n",
    "                request_id,\n",
    "                feature_1, ..., feature_m (where m is coef.shape[1]),\n",
    "                relevance (click probability given that the position was observed),\n",
    "                c (click indicator)\n",
    "    \"\"\"\n",
    "    n_features = len(coef)\n",
    "    feature_names = [\"feature_%i\" % i for i in range(1, n_features + 1)]\n",
    "    data = pd.DataFrame(\n",
    "        np.concatenate(\n",
    "            [\n",
    "                np.repeat(range(n_requests), n_positions)[:, None],\n",
    "                np.array(list(range(1, n_positions + 1)) * n_requests)[:, None],\n",
    "                np.random.normal(0, 1, (n_requests * n_positions, n_features))\n",
    "            ],\n",
    "            axis=1\n",
    "        ),\n",
    "        columns=[\"request_id\", \"position\"] + feature_names\n",
    "    )\n",
    "    view_probs = 1 / np.arange(1, n_positions + 1)\n",
    "    view_probs = view_probs / np.concatenate([[1.0], view_probs[:-1]])\n",
    "    view_indicators = np.random.binomial(1, list(view_probs) * n_requests)\n",
    "    z = np.dot(data[feature_names].values, coef) - 4.0\n",
    "    data[\"relevance\"] = 1 / (1 + np.exp(-z))\n",
    "    c = np.random.binomial(1, data.relevance)\n",
    "    data[\"c\"] = np.zeros(len(data))\n",
    "    for i in range(n_requests):\n",
    "        for j in range(n_positions):\n",
    "            idx = i * n_positions + j\n",
    "            if not view_indicators[idx]:\n",
    "                break\n",
    "            data[\"c\"].iloc[idx] = c[idx]\n",
    "    data[\"viewed\"] = view_indicators\n",
    "    data[\"_c\"] = c\n",
    "    return data\n",
    "\n",
    "\n",
    "def drop_requests_with_no_interactions(data, interaction_col):\n",
    "    interaction_requests = set(data.loc[data[interaction_col] > 0].request_id)\n",
    "    return data.loc[data.request_id.isin(interaction_requests)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "COEF = [1.0, -1.0]\n",
    "N_POSITIONS = 10\n",
    "MAX_NDCG_POS = 10\n",
    "N_TRAIN = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = generate_data(N_POSITIONS, [1.0, -1.0], N_TRAIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "    \"num_trees\": 10,\n",
    "    \"objective\": \"lambdarank\",\n",
    "    \"max_position\": MAX_NDCG_POS, \n",
    "    \"metric\": \"ndcg\",\n",
    "    \"eval_at\": MAX_NDCG_POS\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetWithCalculatorAndRanks0(lgb.Dataset):\n",
    "    def __init__(self, max_ndcg_pos, ranks, *args, **kwargs):\n",
    "            lgb.Dataset.__init__(self, *args, **kwargs)\n",
    "            self.calculator = Calculator(self.label, self.get_group(), max_ndcg_pos)\n",
    "            self.ranks0 = (ranks - 1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = drop_requests_with_no_interactions(train_data, \"c\")\n",
    "train_dataset = DatasetWithCalculatorAndRanks0(\n",
    "    MAX_NDCG_POS,\n",
    "    train_data.position.values,\n",
    "    train_data.drop([\"request_id\", \"position\", \"c\", \"relevance\", \"viewed\", \"_c\"], axis=1),\n",
    "    label=train_data.c.values,\n",
    "    group=[N_POSITIONS] * train_data.request_id.nunique(),\n",
    "    free_raw_data=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With a biased objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient_for_one_query(\n",
    "    gains,\n",
    "    preds,\n",
    "    start,\n",
    "    end,\n",
    "    inverse_max_dcg,\n",
    "    grad, \n",
    "    hess, \n",
    "    calculator\n",
    "):\n",
    "    sorted_idx = np.argsort(-preds[start:end])\n",
    "    should_adjust = preds[start + sorted_idx[0]] != preds[start + sorted_idx[-1]]\n",
    "    for i in range(len(sorted_idx)):\n",
    "        high = sorted_idx[i]\n",
    "        gain_high = gains[start + high]\n",
    "        for j in range(len(sorted_idx)):\n",
    "            low = sorted_idx[j]\n",
    "            gain_low = gains[start + low]\n",
    "            if gain_high > gain_low:\n",
    "                score_high = preds[start + high]\n",
    "                score_low = preds[start + low]\n",
    "                score_diff = score_high - score_low\n",
    "                p_lambda = calculator.get_sigmoid(score_diff)\n",
    "                p_hess = p_lambda * (2.0 - p_lambda)\n",
    "                gain_diff = gain_high - gain_low\n",
    "                paired_discount = calculator.discounts[1 + i] - calculator.discounts[1 + j]\n",
    "                abs_delta_ndcg = np.abs(gain_diff * paired_discount * inverse_max_dcg)\n",
    "                if should_adjust == 1:\n",
    "                    abs_delta_ndcg /= (0.01 + np.abs(score_diff))\n",
    "                p_lambda *= -abs_delta_ndcg\n",
    "                p_hess *= 2 * abs_delta_ndcg\n",
    "    \n",
    "                grad[start + high] += p_lambda\n",
    "                grad[start + low] -= p_lambda\n",
    "                hess[start + high] += p_hess\n",
    "                hess[start + low] += p_hess\n",
    "                \n",
    "                \n",
    "def get_gradients(gains, preds, groups, grad_for_one_query_func, calculator):\n",
    "    grad = [0] * len(gains)\n",
    "    hess = [0] * len(gains)\n",
    "    \n",
    "    query_boundaries = calculator.query_boundaries   \n",
    "\n",
    "    for i in range(len(query_boundaries) - 1):\n",
    "        grad_for_one_query_func(gains, \n",
    "                                preds, \n",
    "                                query_boundaries[i], \n",
    "                                query_boundaries[i + 1], \n",
    "                                calculator.inverse_max_dcgs[i],\n",
    "                                grad, \n",
    "                                hess,\n",
    "                                calculator)\n",
    "\n",
    "    return grad, hess\n",
    "\n",
    "\n",
    "def lambdarank_objective(preds, train_data):\n",
    "    groups = train_data.get_group()\n",
    "    calculator = train_data.calculator\n",
    "    \n",
    "    if len(groups) == 0:\n",
    "        raise Error(\"Group/query data should not be empty.\")\n",
    "    else:\n",
    "        grad, hess = get_gradients(\n",
    "            train_data.label,\n",
    "            preds,\n",
    "            groups,\n",
    "            get_gradient_for_one_query,\n",
    "            calculator\n",
    "        )\n",
    "\n",
    "        return grad, hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's ndcg@10: 0.727797\n",
      "[2]\ttraining's ndcg@10: 0.72946\n",
      "[3]\ttraining's ndcg@10: 0.730345\n",
      "[4]\ttraining's ndcg@10: 0.730983\n",
      "[5]\ttraining's ndcg@10: 0.731797\n",
      "[6]\ttraining's ndcg@10: 0.732035\n",
      "[7]\ttraining's ndcg@10: 0.732415\n",
      "[8]\ttraining's ndcg@10: 0.732267\n",
      "[9]\ttraining's ndcg@10: 0.732783\n",
      "[10]\ttraining's ndcg@10: 0.733984\n"
     ]
    }
   ],
   "source": [
    "def fit_original(dataset, verbose_eval=True):\n",
    "    lgb.train(\n",
    "        params=lgb_params, \n",
    "        train_set=dataset,\n",
    "        valid_sets=[dataset],\n",
    "        verbose_eval=verbose_eval\n",
    "    )    \n",
    "\n",
    "\n",
    "fit_original(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's ndcg@10: 0.727797\n",
      "[2]\ttraining's ndcg@10: 0.72946\n",
      "[3]\ttraining's ndcg@10: 0.730345\n",
      "[4]\ttraining's ndcg@10: 0.730983\n",
      "[5]\ttraining's ndcg@10: 0.731797\n",
      "[6]\ttraining's ndcg@10: 0.732035\n",
      "[7]\ttraining's ndcg@10: 0.732415\n",
      "[8]\ttraining's ndcg@10: 0.732267\n",
      "[9]\ttraining's ndcg@10: 0.732783\n",
      "[10]\ttraining's ndcg@10: 0.733984\n"
     ]
    }
   ],
   "source": [
    "def fit_custom_objective(dataset, verbose_eval=True):\n",
    "    lgb.train(\n",
    "        params=lgb_params, \n",
    "        train_set=dataset,\n",
    "        valid_sets=[dataset],\n",
    "        verbose_eval=verbose_eval,\n",
    "        fobj=lambdarank_objective\n",
    "    )    \n",
    "\n",
    "\n",
    "fit_custom_objective(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a python implementation of the unbiased objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With updating t_plus and t_minus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unbiased_gradient_for_one_query(\n",
    "    gains,\n",
    "    preds,\n",
    "    cur_t_plus,\n",
    "    cur_t_minus,\n",
    "    next_t_plus,\n",
    "    next_t_minus,\n",
    "    ranks0,\n",
    "    start, \n",
    "    end,\n",
    "    inverse_max_dcg,\n",
    "    grad, \n",
    "    hess, \n",
    "    calculator\n",
    "):\n",
    "    sorted_idx = np.argsort(-preds[start:end])\n",
    "    should_adjust = preds[start + sorted_idx[0]] != preds[start + sorted_idx[-1]]\n",
    "    for i in range(len(sorted_idx)):\n",
    "        high = sorted_idx[i]\n",
    "        gain_high = gains[start + high]\n",
    "        for j in range(len(sorted_idx)):\n",
    "            low = sorted_idx[j]\n",
    "            gain_low = gains[start + low]\n",
    "            if gain_high > gain_low:\n",
    "                score_high = preds[start + high]\n",
    "                score_low = preds[start + low]\n",
    "                rank_high = ranks0[start + high]\n",
    "                rank_low = ranks0[start + low]\n",
    "\n",
    "                score_diff = score_high - score_low\n",
    "                p_lambda = calculator.get_sigmoid(score_diff)\n",
    "                p_hess = p_lambda * (2.0 - p_lambda)\n",
    "                gain_diff = gain_high - gain_low\n",
    "                paired_discount = calculator.discounts[1 + i] - calculator.discounts[1 + j]\n",
    "                abs_delta_ndcg = np.abs(gain_diff * paired_discount * inverse_max_dcg)\n",
    "                if should_adjust == 1:\n",
    "                    abs_delta_ndcg /= (0.01 + np.abs(score_diff))\n",
    "                p_lambda *= -abs_delta_ndcg\n",
    "                p_hess *= 2 * abs_delta_ndcg\n",
    "                \n",
    "                p_lambda /= cur_t_plus[rank_high] * cur_t_minus[rank_low]\n",
    "                p_hess /= cur_t_plus[rank_high] * cur_t_minus[rank_low]\n",
    "    \n",
    "                grad[start + high] += p_lambda\n",
    "                grad[start + low] -= p_lambda\n",
    "                hess[start + high] += p_hess\n",
    "                hess[start + low] += p_hess\n",
    "                \n",
    "                loss = calculator.get_log(score_diff) * abs_delta_ndcg\n",
    "                next_t_plus[rank_high] += loss / cur_t_minus[rank_low]\n",
    "                next_t_minus[rank_low] += loss / cur_t_plus[rank_high]  \n",
    "                \n",
    "                \n",
    "def get_unbiased_gradients(\n",
    "    gains, \n",
    "    preds, \n",
    "    cur_t_plus, \n",
    "    cur_t_minus,\n",
    "    ranks0,\n",
    "    groups, \n",
    "    grad_for_one_query_func, \n",
    "    calculator,\n",
    "    p\n",
    "):\n",
    "    grad = [0] * len(gains)\n",
    "    hess = [0] * len(gains)\n",
    "    next_t_plus = [0] * len(cur_t_plus)\n",
    "    next_t_minus = [0] * len(cur_t_minus)\n",
    "    \n",
    "    query_boundaries = calculator.query_boundaries   \n",
    "\n",
    "    for i in range(len(query_boundaries) - 1):\n",
    "        grad_for_one_query_func(gains, \n",
    "                                preds,\n",
    "                                cur_t_plus,\n",
    "                                cur_t_minus,\n",
    "                                next_t_plus,\n",
    "                                next_t_minus,\n",
    "                                ranks0,\n",
    "                                query_boundaries[i], \n",
    "                                query_boundaries[i + 1], \n",
    "                                calculator.inverse_max_dcgs[i],\n",
    "                                grad, \n",
    "                                hess,\n",
    "                                calculator)\n",
    "    \n",
    "    next_t_plus = np.power(next_t_plus / next_t_plus[0], 1 / (1 + p))\n",
    "    next_t_minus = np.power(next_t_minus / next_t_minus[0], 1 / (1 + p))\n",
    "    \n",
    "    return grad, hess, next_t_plus, next_t_minus\n",
    "\n",
    "\n",
    "def unbiased_lambdarank_objective(preds, train_data):\n",
    "    \"\"\"\n",
    "    Uses global variables t_plus and t_minus.\n",
    "    \"\"\"\n",
    "    global t_plus, t_minus\n",
    "    \n",
    "    groups = train_data.get_group()\n",
    "\n",
    "    if len(groups) == 0:\n",
    "        raise Error(\"Group/query data should not be empty.\")\n",
    "    else:\n",
    "        grad, hess, next_t_plus, next_t_minus = get_unbiased_gradients(\n",
    "            train_data.label,\n",
    "            preds,\n",
    "            t_plus,\n",
    "            t_minus,\n",
    "            train_data.ranks0,\n",
    "            groups,\n",
    "            get_unbiased_gradient_for_one_query,\n",
    "            train_data.calculator,\n",
    "            0.0\n",
    "        )\n",
    "        t_plus, t_minus = next_t_plus, next_t_minus\n",
    "        \n",
    "        return grad, hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's ndcg@10: 0.727797\n",
      "[2]\ttraining's ndcg@10: 0.72874\n",
      "[3]\ttraining's ndcg@10: 0.729093\n",
      "[4]\ttraining's ndcg@10: 0.729409\n",
      "[5]\ttraining's ndcg@10: 0.728661\n",
      "[6]\ttraining's ndcg@10: 0.72956\n",
      "[7]\ttraining's ndcg@10: 0.730146\n",
      "[8]\ttraining's ndcg@10: 0.729872\n",
      "[9]\ttraining's ndcg@10: 0.730273\n",
      "[10]\ttraining's ndcg@10: 0.730882\n",
      "[1.         0.50126325 0.31899954 0.24712254 0.20910728 0.15146912\n",
      " 0.13427764 0.11128859 0.08859075 0.08483076] [1.         1.0096045  0.9788292  0.99767647 0.96291405 0.9822002\n",
      " 0.96958388 1.01251736 1.02118098 1.0175265 ]\n"
     ]
    }
   ],
   "source": [
    "def fit_custom_objective(dataset, verbose_eval=True):\n",
    "    lgb.train(\n",
    "        params=lgb_params, \n",
    "        train_set=dataset,\n",
    "        valid_sets=[dataset],\n",
    "        verbose_eval=verbose_eval,\n",
    "        fobj=unbiased_lambdarank_objective\n",
    "    )    \n",
    "\n",
    "\n",
    "t_plus = np.ones(train_dataset.calculator.max_rank)\n",
    "t_minus = np.ones(train_dataset.calculator.max_rank)\n",
    "fit_custom_objective(train_dataset)\n",
    "print(t_plus, t_minus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using fixed t_plus and t_minus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unbiased_gradient_for_one_query(\n",
    "    gains,\n",
    "    preds,\n",
    "    t_plus,\n",
    "    t_minus,\n",
    "    ranks0,\n",
    "    start, \n",
    "    end,\n",
    "    inverse_max_dcg,\n",
    "    grad, \n",
    "    hess, \n",
    "    calculator\n",
    "):\n",
    "    sorted_idx = np.argsort(-preds[start:end])\n",
    "    should_adjust = preds[start + sorted_idx[0]] != preds[start + sorted_idx[-1]]\n",
    "    for i in range(len(sorted_idx)):\n",
    "        high = sorted_idx[i]\n",
    "        gain_high = gains[start + high]\n",
    "        for j in range(len(sorted_idx)):\n",
    "            low = sorted_idx[j]\n",
    "            gain_low = gains[start + low]\n",
    "            if gain_high > gain_low:\n",
    "                score_high = preds[start + high]\n",
    "                score_low = preds[start + low]\n",
    "                rank_high = ranks0[start + high]\n",
    "                rank_low = ranks0[start + low]\n",
    "\n",
    "                score_diff = score_high - score_low\n",
    "                p_lambda = calculator.get_sigmoid(score_diff)\n",
    "                p_hess = p_lambda * (2.0 - p_lambda)\n",
    "                gain_diff = gain_high - gain_low\n",
    "                paired_discount = calculator.discounts[1 + i] - calculator.discounts[1 + j]\n",
    "                abs_delta_ndcg = np.abs(gain_diff * paired_discount * inverse_max_dcg)\n",
    "                if should_adjust == 1:\n",
    "                    abs_delta_ndcg /= (0.01 + np.abs(score_diff))\n",
    "                p_lambda *= -abs_delta_ndcg\n",
    "                p_hess *= 2 * abs_delta_ndcg\n",
    "                \n",
    "                p_lambda /= t_plus[rank_high] * t_minus[rank_low]\n",
    "                p_hess /= t_plus[rank_high] * t_minus[rank_low]\n",
    "    \n",
    "                grad[start + high] += p_lambda\n",
    "                grad[start + low] -= p_lambda\n",
    "                hess[start + high] += p_hess\n",
    "                hess[start + low] += p_hess  \n",
    "                \n",
    "                \n",
    "def get_unbiased_gradients(\n",
    "    gains, \n",
    "    preds, \n",
    "    t_plus, \n",
    "    t_minus,\n",
    "    ranks0,\n",
    "    groups, \n",
    "    grad_for_one_query_func, \n",
    "    calculator\n",
    "):\n",
    "    grad = [0] * len(gains)\n",
    "    hess = [0] * len(gains)\n",
    "    \n",
    "    query_boundaries = calculator.query_boundaries   \n",
    "\n",
    "    for i in range(len(query_boundaries) - 1):\n",
    "        grad_for_one_query_func(gains, \n",
    "                                preds,\n",
    "                                t_plus,\n",
    "                                t_minus,\n",
    "                                ranks0,\n",
    "                                query_boundaries[i], \n",
    "                                query_boundaries[i + 1], \n",
    "                                calculator.inverse_max_dcgs[i],\n",
    "                                grad, \n",
    "                                hess,\n",
    "                                calculator)\n",
    "        \n",
    "    return grad, hess\n",
    "\n",
    "\n",
    "def unbiased_lambdarank_objective(preds, train_data):\n",
    "    \"\"\"\n",
    "    Uses global variables t_plus and t_minus.\n",
    "    \"\"\"\n",
    "    global t_plus, t_minus\n",
    "    \n",
    "    groups = train_data.get_group()\n",
    "    \n",
    "    if len(groups) == 0:\n",
    "        raise Error(\"Group/query data should not be empty.\")\n",
    "    else:\n",
    "        grad, hess = get_unbiased_gradients(\n",
    "            train_data.label,\n",
    "            preds,\n",
    "            t_plus,\n",
    "            t_minus,\n",
    "            train_data.ranks0,\n",
    "            groups,\n",
    "            get_unbiased_gradient_for_one_query,\n",
    "            train_data.calculator\n",
    "        )\n",
    "        \n",
    "        return grad, hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's ndcg@10: 0.72752\n",
      "[2]\ttraining's ndcg@10: 0.727348\n",
      "[3]\ttraining's ndcg@10: 0.72688\n",
      "[4]\ttraining's ndcg@10: 0.729711\n",
      "[5]\ttraining's ndcg@10: 0.729438\n",
      "[6]\ttraining's ndcg@10: 0.728955\n",
      "[7]\ttraining's ndcg@10: 0.729402\n",
      "[8]\ttraining's ndcg@10: 0.730952\n",
      "[9]\ttraining's ndcg@10: 0.731938\n",
      "[10]\ttraining's ndcg@10: 0.731942\n"
     ]
    }
   ],
   "source": [
    "def fit_custom_objective(dataset, verbose_eval=True):\n",
    "    lgb.train(\n",
    "        params=lgb_params, \n",
    "        train_set=dataset,\n",
    "        valid_sets=[dataset],\n",
    "        verbose_eval=verbose_eval,\n",
    "        fobj=unbiased_lambdarank_objective\n",
    "    )    \n",
    "\n",
    "\n",
    "t_plus = 1 / np.arange(1, train_dataset.calculator.max_rank + 1)\n",
    "t_minus = np.ones(train_dataset.calculator.max_rank)\n",
    "fit_custom_objective(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a cython implementation of the unbiased objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With updating t_plus and t_minus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unbiasedlambdamart.lambdaobj import get_unbiased_gradients\n",
    "from unbiasedlambdamart.calculator import MIN_ARG, MAX_ARG\n",
    "from unbiasedlambdamart.objective import DatasetWithCalculatorRanks0AndP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unbiased_lambdarank_objective(preds, dataset):\n",
    "    \"\"\"\n",
    "    Uses global variables t_plus and t_minus.\n",
    "    \"\"\"\n",
    "    global t_plus, t_minus\n",
    "    \n",
    "    groups = dataset.get_group()\n",
    "    \n",
    "    if len(groups) == 0:\n",
    "        raise Error(\"Group/query data should not be empty.\")\n",
    "    else:\n",
    "        grad = np.zeros(len(preds))\n",
    "        hess = np.zeros(len(preds))\n",
    "        get_unbiased_gradients(\n",
    "            np.ascontiguousarray(dataset.label, dtype=np.double), \n",
    "            np.ascontiguousarray(preds),\n",
    "            np.ascontiguousarray(dataset.ranks0),\n",
    "            len(preds),\n",
    "            np.ascontiguousarray(groups),\n",
    "            np.ascontiguousarray(dataset.calculator.query_boundaries),\n",
    "            len(dataset.calculator.query_boundaries) - 1,\n",
    "            np.ascontiguousarray(dataset.calculator.discounts),\n",
    "            np.ascontiguousarray(dataset.calculator.inverse_max_dcgs),\n",
    "            np.ascontiguousarray(dataset.calculator.sigmoids),\n",
    "            len(dataset.calculator.sigmoids),\n",
    "            MIN_ARG,\n",
    "            MAX_ARG,\n",
    "            dataset.calculator.idx_factor,\n",
    "            np.ascontiguousarray(dataset.calculator.logs),\n",
    "            len(dataset.calculator.logs),\n",
    "            MIN_ARG,\n",
    "            MAX_ARG,\n",
    "            dataset.calculator.idx_factor,\n",
    "            dataset.p,\n",
    "            dataset.calculator.max_rank,\n",
    "            np.ascontiguousarray(grad), \n",
    "            np.ascontiguousarray(hess),\n",
    "            np.ascontiguousarray(t_plus),\n",
    "            np.ascontiguousarray(t_minus)\n",
    "        )\n",
    "        \n",
    "        return grad, hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's ndcg@10: 0.727797\n",
      "[2]\ttraining's ndcg@10: 0.72874\n",
      "[3]\ttraining's ndcg@10: 0.729093\n",
      "[4]\ttraining's ndcg@10: 0.729409\n",
      "[5]\ttraining's ndcg@10: 0.728661\n",
      "[6]\ttraining's ndcg@10: 0.72956\n",
      "[7]\ttraining's ndcg@10: 0.730146\n",
      "[8]\ttraining's ndcg@10: 0.729872\n",
      "[9]\ttraining's ndcg@10: 0.730273\n",
      "[10]\ttraining's ndcg@10: 0.730882\n",
      "[1.         0.50126325 0.31899954 0.24712254 0.20910728 0.15146912\n",
      " 0.13427764 0.11128859 0.08859075 0.08483076] [1.         1.0096045  0.9788292  0.99767647 0.96291405 0.9822002\n",
      " 0.96958388 1.01251736 1.02118098 1.0175265 ]\n"
     ]
    }
   ],
   "source": [
    "def fit_custom_objective(dataset, verbose_eval=True):\n",
    "    lgb.train(\n",
    "        params=lgb_params, \n",
    "        train_set=dataset,\n",
    "        valid_sets=[dataset],\n",
    "        verbose_eval=verbose_eval,\n",
    "        fobj=unbiased_lambdarank_objective\n",
    "    )    \n",
    "\n",
    "train_dataset = DatasetWithCalculatorRanks0AndP(\n",
    "    MAX_NDCG_POS,\n",
    "    train_data.position.values,\n",
    "    0.0,\n",
    "    data=train_data.drop([\"request_id\", \"position\", \"c\", \"relevance\", \"viewed\", \"_c\"], axis=1),\n",
    "    label=train_data.c.values,\n",
    "    group=[N_POSITIONS] * train_data.request_id.nunique(),\n",
    "    free_raw_data=False\n",
    ")\n",
    "\n",
    "t_plus = np.ones(train_dataset.calculator.max_rank)\n",
    "t_minus = np.ones(train_dataset.calculator.max_rank)\n",
    "fit_custom_objective(train_dataset)\n",
    "print(t_plus, t_minus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using fixed t_plus and t_minus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unbiasedlambdamart.lambdaobj import get_unbiased_gradients_fixed_t\n",
    "from unbiasedlambdamart.calculator import MIN_ARG, MAX_ARG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unbiased_lambdarank_objective_fixed_t(preds, dataset):\n",
    "    \"\"\"\n",
    "    Uses global variables t_plus and t_minus.\n",
    "    \"\"\"\n",
    "    global t_plus, t_minus\n",
    "    \n",
    "    groups = dataset.get_group()\n",
    "    \n",
    "    if len(groups) == 0:\n",
    "        raise Error(\"Group/query data should not be empty.\")\n",
    "    else:\n",
    "        grad = np.zeros(len(preds))\n",
    "        hess = np.zeros(len(preds))\n",
    "        get_unbiased_gradients_fixed_t(\n",
    "            np.ascontiguousarray(dataset.label, dtype=np.double), \n",
    "            np.ascontiguousarray(preds),\n",
    "            np.ascontiguousarray(t_plus),\n",
    "            np.ascontiguousarray(t_minus),\n",
    "            np.ascontiguousarray(dataset.ranks0),\n",
    "            len(preds),\n",
    "            np.ascontiguousarray(groups),\n",
    "            np.ascontiguousarray(dataset.calculator.query_boundaries),\n",
    "            len(dataset.calculator.query_boundaries) - 1,\n",
    "            np.ascontiguousarray(dataset.calculator.discounts),\n",
    "            np.ascontiguousarray(dataset.calculator.inverse_max_dcgs),\n",
    "            np.ascontiguousarray(dataset.calculator.sigmoids),\n",
    "            len(dataset.calculator.sigmoids),\n",
    "            MIN_ARG,\n",
    "            MAX_ARG,\n",
    "            dataset.calculator.idx_factor,\n",
    "            np.ascontiguousarray(grad), \n",
    "            np.ascontiguousarray(hess)\n",
    "        )\n",
    "        \n",
    "        return grad, hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's ndcg@10: 0.72752\n",
      "[2]\ttraining's ndcg@10: 0.727348\n",
      "[3]\ttraining's ndcg@10: 0.72688\n",
      "[4]\ttraining's ndcg@10: 0.729711\n",
      "[5]\ttraining's ndcg@10: 0.729438\n",
      "[6]\ttraining's ndcg@10: 0.728955\n",
      "[7]\ttraining's ndcg@10: 0.729402\n",
      "[8]\ttraining's ndcg@10: 0.730952\n",
      "[9]\ttraining's ndcg@10: 0.731938\n",
      "[10]\ttraining's ndcg@10: 0.731942\n"
     ]
    }
   ],
   "source": [
    "def fit_custom_objective(dataset, verbose_eval=True):\n",
    "    lgb.train(\n",
    "        params=lgb_params, \n",
    "        train_set=dataset,\n",
    "        valid_sets=[dataset],\n",
    "        verbose_eval=verbose_eval,\n",
    "        fobj=unbiased_lambdarank_objective_fixed_t\n",
    "    )    \n",
    "\n",
    "\n",
    "t_plus = 1 / np.arange(1, train_dataset.calculator.max_rank + 1)\n",
    "t_minus = np.ones(train_dataset.calculator.max_rank)\n",
    "fit_custom_objective(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using unbiasedlambdamart.objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With updating t_plus and t_minus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unbiasedlambdamart.objective import unbiased_lambdarank_objective\n",
    "from unbiasedlambdamart.objective import DatasetWithCalculatorRanks0AndP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = drop_requests_with_no_interactions(train_data, \"c\")\n",
    "train_dataset = DatasetWithCalculatorRanks0AndP(\n",
    "    MAX_NDCG_POS,\n",
    "    train_data.position.values,\n",
    "    0.0,\n",
    "    data=train_data.drop([\"request_id\", \"position\", \"c\", \"relevance\", \"viewed\", \"_c\"], axis=1),\n",
    "    label=train_data.c.values,\n",
    "    group=[N_POSITIONS] * train_data.request_id.nunique(),\n",
    "    free_raw_data=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's ndcg@10: 0.727797\n",
      "[2]\ttraining's ndcg@10: 0.72874\n",
      "[3]\ttraining's ndcg@10: 0.729093\n",
      "[4]\ttraining's ndcg@10: 0.729409\n",
      "[5]\ttraining's ndcg@10: 0.728661\n",
      "[6]\ttraining's ndcg@10: 0.72956\n",
      "[7]\ttraining's ndcg@10: 0.730146\n",
      "[8]\ttraining's ndcg@10: 0.729872\n",
      "[9]\ttraining's ndcg@10: 0.730273\n",
      "[10]\ttraining's ndcg@10: 0.730882\n",
      "[1.         0.50126325 0.31899954 0.24712254 0.20910728 0.15146912\n",
      " 0.13427764 0.11128859 0.08859075 0.08483076] [1.         1.0096045  0.9788292  0.99767647 0.96291405 0.9822002\n",
      " 0.96958388 1.01251736 1.02118098 1.0175265 ]\n"
     ]
    }
   ],
   "source": [
    "def fit_custom_objective(dataset, verbose_eval=True):\n",
    "    lgb.train(\n",
    "        params=lgb_params, \n",
    "        train_set=dataset,\n",
    "        valid_sets=[dataset],\n",
    "        verbose_eval=verbose_eval,\n",
    "        fobj=unbiased_lambdarank_objective\n",
    "    )    \n",
    "\n",
    "\n",
    "fit_custom_objective(train_dataset)\n",
    "print(train_dataset.t_plus, train_dataset.t_minus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using fixed t_plus and t_minus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unbiasedlambdamart.objective import unbiased_lambdarank_objective_fixed_t\n",
    "from unbiasedlambdamart.objective import DatasetWithCalculatorRanks0AndT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = drop_requests_with_no_interactions(train_data, \"c\")\n",
    "train_dataset = DatasetWithCalculatorRanks0AndT(\n",
    "    MAX_NDCG_POS,\n",
    "    train_data.position.values,\n",
    "    1.0,\n",
    "    data=train_data.drop([\"request_id\", \"position\", \"c\", \"relevance\", \"viewed\", \"_c\"], axis=1),\n",
    "    label=train_data.c.values,\n",
    "    group=[N_POSITIONS] * train_data.request_id.nunique(),\n",
    "    free_raw_data=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's ndcg@10: 0.72752\n",
      "[2]\ttraining's ndcg@10: 0.727348\n",
      "[3]\ttraining's ndcg@10: 0.72688\n",
      "[4]\ttraining's ndcg@10: 0.729711\n",
      "[5]\ttraining's ndcg@10: 0.729438\n",
      "[6]\ttraining's ndcg@10: 0.728955\n",
      "[7]\ttraining's ndcg@10: 0.729402\n",
      "[8]\ttraining's ndcg@10: 0.730952\n",
      "[9]\ttraining's ndcg@10: 0.731938\n",
      "[10]\ttraining's ndcg@10: 0.731942\n"
     ]
    }
   ],
   "source": [
    "def fit_custom_objective(dataset, verbose_eval=True):\n",
    "    lgb.train(\n",
    "        params=lgb_params, \n",
    "        train_set=dataset,\n",
    "        valid_sets=[dataset],\n",
    "        verbose_eval=verbose_eval,\n",
    "        fobj=unbiased_lambdarank_objective_fixed_t\n",
    "    )    \n",
    "\n",
    "\n",
    "fit_custom_objective(train_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
